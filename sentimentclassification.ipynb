{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6476,"status":"ok","timestamp":1640521013336,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"},"user_tz":-420},"id":"oxUxqxbihyDM","outputId":"40773ca9-551c-4b75-c2a6-a65dc1d24c09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QHEhpLRh_t2"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import transformers\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchsummary import summary\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TJ12yR6iPV5"},"outputs":[],"source":["class BertDataset(Dataset):\n","  def __init__(self, tokenizer, max_length):\n","    super(BertDataset, self).__init__()\n","    self.train_csv = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n","    self.tokenizer = tokenizer\n","    self.n = self.train_csv.shape[0]\n","    self.target = self.train_csv.iloc[:, 1]\n","    self.max_length = max_length\n","\n","  def __len__(self):\n","    return len(self.train_csv)\n","  def getNumSample(self):\n","    return self.train_csv.shape[0]\n","  def __getitem__(self, index):\n","    text1 = self.train_csv.iloc[index, 0]\n","\n","    inputs = self.tokenizer.encode_plus(\n","        text1,\n","        None,\n","        pad_to_max_length = True,\n","        add_special_tokens =  True,\n","        return_attention_mask = True,\n","        max_length = self.max_length,\n","    )\n","\n","    ids = inputs[\"input_ids\"]\n","    token_type_ids = inputs[\"token_type_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","\n","    return{\n","        \"ids\" : torch.tensor(ids, dtype = torch.long),\n","        \"mask\" : torch.tensor(mask, dtype = torch.long),\n","        \"token_type_ids\":torch.tensor(token_type_ids, dtype = torch.long),\n","        \"target\":torch.tensor(self.train_csv.iloc[index, 1], dtype = torch.long)\n","    }\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wt_lifghkeJw"},"outputs":[],"source":["tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","dataset = BertDataset(tokenizer, max_length = 100)\n","\n","dataloader = DataLoader(dataset = dataset, batch_size = 32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2717,"status":"ok","timestamp":1640521249272,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"},"user_tz":-420},"id":"CQz6vUeak292","outputId":"77398039-a0d3-4a0f-aec1-11c6de7c0ade"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["class BERT(nn.Module):\n","  def __init__(self):\n","    super(BERT, self).__init__()\n","    self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n","    self.out = nn.Linear(768, 1)\n","\n","  def forward(self, ids, mask, token_type_ids):\n","    _,o2 = self.bert_model(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n","\n","    output = self.out(o2)\n","    return output\n","\n","model = BERT()\n","\n","loss_fn = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tihd4-hBk8rM"},"outputs":[],"source":["for param in model.bert_model.parameters():\n","  param.requires_grad = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjL6d_0Bmorg"},"outputs":[],"source":["def finetune(epochs,dataloader,model,loss_fn,optimizer):\n","    model.train()\n","    for  epoch in range(epochs):\n","        print(epoch)\n","        \n","        loop=tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n","        for batch, dl in loop:\n","            ids=dl['ids']\n","            token_type_ids=dl['token_type_ids']\n","            mask= dl['mask']\n","            label=dl['target']\n","            label = label.unsqueeze(1)\n","            \n","            optimizer.zero_grad()\n","            \n","            output=model(\n","                ids=ids,\n","                mask=mask,\n","                token_type_ids = token_type_ids)\n","            label = label.type_as(output)\n","\n","            loss=loss_fn(output,label)\n","            loss.backward()\n","            \n","            optimizer.step()\n","            \n","            pred = np.where(output >= 0, 1, 0)\n","\n","            num_correct = sum(1 for a, b in zip(pred, label) if a[0] == b[0])\n","            num_samples = pred.shape[0]\n","            accuracy = num_correct/num_samples\n","            \n","            print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n","            \n","            # Show progress while training\n","            loop.set_description(f'Epoch={epoch}/{epochs}')\n","            loop.set_postfix(loss=loss.item(),acc=accuracy)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"elapsed":5473,"status":"error","timestamp":1640516310177,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"},"user_tz":-420},"id":"AcvumLAxosOe","outputId":"8cdf5969-f066-4d44-b5e7-b12d1ee0d50f"},"outputs":[],"source":["# model=finetune(5, dataloader, model, loss_fn, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kU6DTIAFbwY0"},"outputs":[],"source":["torch.save(model.state_dict(), \"model1.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":823,"status":"ok","timestamp":1640521254436,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"},"user_tz":-420},"id":"HsZnsPwrawiB","outputId":"d2415745-d1a3-4432-b2f8-efc79560b5f0"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load(\"/content/drive/MyDrive/Fine-Tune-Bert/model1.pth\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wfse_40DACZ"},"outputs":[],"source":["#model= torch.load(\"/content/drive/MyDrive/Fine-Tune-Bert/model1.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":2135,"status":"error","timestamp":1640521082405,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"},"user_tz":-420},"id":"s2yAxJkAVEDJ","outputId":"ed91c581-6951-4de2-a0ec-0ae8c9d8f5d1"},"outputs":[],"source":["loop=tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n","num_samples = dataset.getNumSample()\n","num_correct = 0\n","for batch, dl in loop:\n","  ids=dl['ids']\n","  print(ids.shape)\n","  token_type_ids=dl['token_type_ids']\n","  print(token_type_ids.shape)\n","  mask= dl['mask']\n","  print(mask.shape)\n","  label=dl['target']\n","  label = label.unsqueeze(1)\n","            \n","  output=model(\n","                ids=ids,\n","                mask=mask,\n","                token_type_ids = token_type_ids)\n","  label = label.type_as(output)\n","  pred = np.where(output >= 0, 1, 0)\n","\n","  num_correct += sum(1 for a, b in zip(pred, label) if a[0] == b[0])\n","            \n","print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qZ9YQzgUydu"},"outputs":[],"source":["# def evaluate(dataloader, model, dataset):\n","#   num_correct = 0\n","#   for batch, dl in enumerate(dataloader):\n","#     label = dl['target']\n","#     label = label.unsqueeze(1)\n","#     temp = dl['token_type_ids']\n","#     output = model(\n","#           ids = dl['ids'],\n","#           mask = dl['mask'],\n","#           token_type_ids = temp\n","#     )\n","#     print(batch)\n","\n","#     predict = np.where(output >=0, 1, 0)\n","#     num_correct += sum(1 for a, b in zip(predict, label) if a[0] == b[0])\n","#   result = num_correct / float(dataset.getNumSample())\n","#   return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhKKbSh_9d3r"},"outputs":[],"source":["# print(evaluate(dataloader, model, dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0Ls6iY2VMbE"},"outputs":[],"source":["def predict(text, tokenizer):\n","  inputs = tokenizer.encode_plus(\n","      text,\n","      None,\n","      pad_to_max_length = True,\n","      add_special_tokens =  True,\n","      return_attention_mask = True,\n","      max_length = 100,\n","  )\n","  output = model(\n","        ids = torch.tensor(inputs[\"input_ids\"], dtype = torch.long).reshape(1,-1),\n","        mask = torch.tensor(inputs[\"attention_mask\"], dtype = torch.long).reshape(1,-1),\n","        token_type_ids = torch.tensor(inputs[\"token_type_ids\"], dtype = torch.long).reshape(1,-1)\n","  )\n","  print(\"Nhận xét : \",text)\n","  if(np.where(output >=0, 1, 0)[0][0] == 1):\n","    print(\"dự đoán: positive\")\n","  else:\n","    print(\"dự đoán: negative\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QsLj0QffDjw"},"outputs":[],"source":["text1 = \"I really miss this movie so much. That time when I watched this movie on theatres with full of people and the crowd went crazy during epic fights\"\n","text2 = \"the film is strictly routine\"\n","text3 = \"Endgame was such a legendary movie that people come to re-watch it's trailer 2 years after it's release\"\n","text4 = \"All I can say is.. I'm grateful this movie was released in 2019 to be experienced the way it was meant to be\"\n","text5 = \"the drama discloses almost nothing\"\n","text6 = \"clockstoppers is one of those crazy , mixed up films that does n't know what it wants to be when it grows up\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2816,"status":"ok","timestamp":1640522238213,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"},"user_tz":-420},"id":"-4N65xdbfEAm","outputId":"eba0716b-9173-4007-885b-ed798de4cc57"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["Nhận xét :  I really miss this movie so much. That time when I watched this movie on theatres with full of people and the crowd went crazy during epic fights\n","dự đoán: positive\n","Nhận xét :  the film is strictly routine\n","dự đoán: negative\n","Nhận xét :  Endgame was such a legendary movie that people come to re-watch it's trailer 2 years after it's release\n","dự đoán: positive\n","Nhận xét :  All I can say is.. I'm grateful this movie was released in 2019 to be experienced the way it was meant to be\n","dự đoán: positive\n","Nhận xét :  the drama discloses almost nothing\n","dự đoán: negative\n","Nhận xét :  clockstoppers is one of those crazy , mixed up films that does n't know what it wants to be when it grows up\n","dự đoán: negative\n"]}],"source":["predict(text1,tokenizer)\n","predict(text2,tokenizer)\n","predict(text3,tokenizer)\n","predict(text4,tokenizer)\n","predict(text5,tokenizer)\n","predict(text6,tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMINmplZv_jD"},"outputs":[],"source":["text1 = \"it's not too fast and not too slow\"\n","text2 = \"this movie is normal\"\n","text3 = \"This movie is neither good nor bad\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2550,"status":"ok","timestamp":1640522461341,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"},"user_tz":-420},"id":"Dq_cY55lwCUT","outputId":"3a071385-d17c-42b0-fa73-3cdedd5a93d1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["Nhận xét :  it's not too fast and not too slow\n","dự đoán: negative\n","Nhận xét :  this movie is normal\n","dự đoán: negative\n","Nhận xét :  This movie is neither good nor bad\n","dự đoán: negative\n"]}],"source":["predict(text1,tokenizer)\n","predict(text2,tokenizer)\n","predict(text3,tokenizer)"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyPwQWPZqXkw/+t6Z4iHwEc3","collapsed_sections":[],"mount_file_id":"116R4zOcdKc5OB6zqpEfzvaLO6YGs2mvl","name":"sentimentclassification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
